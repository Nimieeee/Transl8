# Beta Testing Metrics Monitoring Guide

## Overview

This guide explains how to monitor and analyze beta testing metrics to track program success and identify areas for improvement.

## Quick Start

### Generate Metrics Report

```bash
cd packages/backend
npm run beta-metrics display
```

### Export Metrics to JSON

```bash
npm run beta-metrics export beta-metrics-2024-01-15.json
```

### Get Raw JSON Output

```bash
npm run beta-metrics json
```

## Key Metrics

### 1. Activation Metrics

**What to Track:**
- Total invited vs. activated users
- Activation rate (target: 70%+)
- Average time to activation (target: <3 days)

**Success Indicators:**
- ‚úÖ 70%+ activation rate
- ‚úÖ <48 hours average activation time
- ‚úÖ Steady activation growth

**Red Flags:**
- ‚ùå <50% activation rate
- ‚ùå >5 days average activation time
- ‚ùå Declining activation trend

**Actions:**
- Low activation: Send reminder emails, simplify process
- Slow activation: Improve onboarding, add support
- Declining trend: Review messaging, check technical issues

### 2. Feature Adoption

**What to Track:**
- Projects created per user (target: 3+)
- Users with at least one project (target: 50%+)
- Voice clone adoption (target: 30%+)
- Glossary usage (target: 20%+)

**Success Indicators:**
- ‚úÖ 50%+ users create projects
- ‚úÖ 3+ projects per active user
- ‚úÖ 30%+ try voice cloning
- ‚úÖ Growing feature usage over time

**Red Flags:**
- ‚ùå <30% users create projects
- ‚ùå <1 project per user
- ‚ùå <10% feature adoption
- ‚ùå Declining usage

**Actions:**
- Low project creation: Improve tutorials, add incentives
- Low feature adoption: Feature spotlights, tutorials
- Declining usage: Fix bugs, gather feedback

### 3. Project Completion

**What to Track:**
- Project completion rate (target: 80%+)
- Average completion time
- Drop-off points by stage
- Failed vs. abandoned projects

**Success Indicators:**
- ‚úÖ 80%+ completion rate
- ‚úÖ <10 minutes average completion
- ‚úÖ Low drop-off at each stage
- ‚úÖ <5% failure rate

**Red Flags:**
- ‚ùå <60% completion rate
- ‚ùå >20 minutes average completion
- ‚ùå High drop-off at specific stage
- ‚ùå >10% failure rate

**Actions:**
- Low completion: Investigate drop-off points
- Slow completion: Optimize processing pipeline
- High drop-off: Fix stage-specific issues
- High failure: Debug and fix errors

### 4. Quality & Satisfaction

**What to Track:**
- Average feedback rating (target: 8+/10)
- Quality ratings by feature
- Satisfaction distribution
- Would-recommend rate (target: 70%+)

**Success Indicators:**
- ‚úÖ 8+ average rating
- ‚úÖ 7+ for all features
- ‚úÖ 70%+ would recommend
- ‚úÖ Improving trend

**Red Flags:**
- ‚ùå <6 average rating
- ‚ùå <5 for any feature
- ‚ùå <50% would recommend
- ‚ùå Declining trend

**Actions:**
- Low ratings: Prioritize quality improvements
- Feature-specific issues: Focus on that feature
- Low recommendation: Address top pain points
- Declining trend: Emergency response needed

### 5. Support Metrics

**What to Track:**
- Total support tickets (benchmark: <2 per user)
- Open ticket count
- Average response time (target: <4 hours)
- Average resolution time (target: <24 hours)
- Common issue categories

**Success Indicators:**
- ‚úÖ <2 tickets per user
- ‚úÖ <10% open tickets
- ‚úÖ <4 hour response time
- ‚úÖ <24 hour resolution time
- ‚úÖ Decreasing ticket volume

**Red Flags:**
- ‚ùå >3 tickets per user
- ‚ùå >25% open tickets
- ‚ùå >8 hour response time
- ‚ùå >48 hour resolution time
- ‚ùå Increasing ticket volume

**Actions:**
- High volume: Improve documentation, fix bugs
- Slow response: Add support capacity
- Slow resolution: Prioritize common issues
- Increasing volume: Identify root causes

### 6. Usage Patterns

**What to Track:**
- Usage by subscription tier
- Popular language pairs
- Peak usage hours
- Geographic distribution
- Content type distribution

**Insights:**
- Identify most valuable user segments
- Optimize for popular language pairs
- Plan capacity for peak hours
- Understand use cases
- Validate pricing tiers

## Monitoring Schedule

### Daily Monitoring (5 minutes)

**Morning Check:**
```bash
npm run beta-metrics display
```

**Review:**
- New activations (yesterday)
- Projects created (yesterday)
- Open support tickets
- Any critical issues

**Actions:**
- Respond to support tickets
- Follow up with inactive users
- Address urgent issues

### Weekly Deep Dive (30 minutes)

**Monday Morning:**
```bash
npm run beta-metrics export weekly-metrics-$(date +%Y-%m-%d).json
```

**Review:**
- Week-over-week trends
- Feature adoption changes
- Quality ratings
- Support ticket patterns
- User engagement

**Actions:**
- Update team on progress
- Adjust strategy if needed
- Plan week's focus areas
- Schedule follow-ups

### Monthly Comprehensive Review (2 hours)

**First Monday of Month:**

**Generate Reports:**
```bash
npm run beta-metrics export monthly-metrics-$(date +%Y-%m).json
```

**Analyze:**
- Month-over-month trends
- Cohort analysis
- Feature performance
- Quality improvements
- ROI assessment

**Actions:**
- Present to stakeholders
- Update roadmap priorities
- Recognize top contributors
- Plan next month's initiatives

## Metric Thresholds

### Critical Thresholds (Immediate Action Required)

| Metric | Threshold | Action |
|--------|-----------|--------|
| Activation Rate | <40% | Emergency review of onboarding |
| Completion Rate | <50% | Stop new invites, fix issues |
| Avg Rating | <5/10 | Emergency quality review |
| Open Tickets | >50 | Add support capacity |
| Response Time | >24 hours | Escalate support |

### Warning Thresholds (Action Needed Soon)

| Metric | Threshold | Action |
|--------|-----------|--------|
| Activation Rate | 40-60% | Review and improve onboarding |
| Completion Rate | 50-70% | Investigate drop-off points |
| Avg Rating | 5-7/10 | Prioritize quality improvements |
| Open Tickets | 25-50 | Review support process |
| Response Time | 8-24 hours | Optimize support workflow |

### Target Thresholds (Healthy)

| Metric | Threshold | Action |
|--------|-----------|--------|
| Activation Rate | >70% | Maintain current approach |
| Completion Rate | >80% | Continue optimizing |
| Avg Rating | >8/10 | Celebrate and maintain |
| Open Tickets | <10 | Current support adequate |
| Response Time | <4 hours | Excellent support |

## Custom Queries

### Find Inactive Beta Testers

```sql
SELECT 
  u.email,
  u.beta_onboarded_at,
  COUNT(p.id) as project_count,
  MAX(p.created_at) as last_project
FROM users u
LEFT JOIN projects p ON p.user_id = u.id
WHERE u.is_beta_tester = true
  AND u.beta_onboarded_at IS NOT NULL
GROUP BY u.id, u.email, u.beta_onboarded_at
HAVING COUNT(p.id) = 0
  OR MAX(p.created_at) < NOW() - INTERVAL '7 days'
ORDER BY u.beta_onboarded_at DESC;
```

### Find Power Users

```sql
SELECT 
  u.email,
  COUNT(DISTINCT p.id) as projects,
  COUNT(DISTINCT f.id) as feedback,
  COUNT(DISTINCT vc.id) as voice_clones,
  AVG(f.rating) as avg_rating
FROM users u
LEFT JOIN projects p ON p.user_id = u.id
LEFT JOIN feedback f ON f.user_id = u.id
LEFT JOIN voice_clones vc ON vc.user_id = u.id
WHERE u.is_beta_tester = true
GROUP BY u.id, u.email
HAVING COUNT(DISTINCT p.id) >= 5
ORDER BY projects DESC, feedback DESC
LIMIT 20;
```

### Analyze Drop-off Points

```sql
SELECT 
  status,
  COUNT(*) as count,
  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
FROM projects
WHERE user_id IN (
  SELECT id FROM users WHERE is_beta_tester = true
)
GROUP BY status
ORDER BY count DESC;
```

### Track Weekly Engagement

```sql
SELECT 
  DATE_TRUNC('week', created_at) as week,
  COUNT(DISTINCT user_id) as active_users,
  COUNT(*) as projects_created,
  ROUND(AVG(EXTRACT(EPOCH FROM (updated_at - created_at)) / 60), 2) as avg_completion_minutes
FROM projects
WHERE user_id IN (
  SELECT id FROM users WHERE is_beta_tester = true
)
GROUP BY DATE_TRUNC('week', created_at)
ORDER BY week DESC;
```

### Identify Common Issues

```sql
SELECT 
  category,
  COUNT(*) as ticket_count,
  ROUND(AVG(EXTRACT(EPOCH FROM (updated_at - created_at)) / 3600), 2) as avg_resolution_hours,
  COUNT(CASE WHEN status IN ('OPEN', 'IN_PROGRESS') THEN 1 END) as open_count
FROM support_tickets
WHERE user_id IN (
  SELECT id FROM users WHERE is_beta_tester = true
)
GROUP BY category
ORDER BY ticket_count DESC;
```

## Automated Alerts

### Set Up Email Alerts

Create a monitoring script that runs hourly:

```bash
#!/bin/bash
# beta-alerts.sh

# Generate metrics
METRICS=$(npm run beta-metrics json)

# Check critical thresholds
ACTIVATION_RATE=$(echo $METRICS | jq '.activation.activationRate')
COMPLETION_RATE=$(echo $METRICS | jq '.projectCompletion.completionRate')
AVG_RATING=$(echo $METRICS | jq '.qualitySatisfaction.avgFeedbackRating')

# Send alerts if thresholds breached
if (( $(echo "$ACTIVATION_RATE < 40" | bc -l) )); then
  echo "CRITICAL: Activation rate below 40%: $ACTIVATION_RATE%" | mail -s "Beta Alert: Low Activation" team@example.com
fi

if (( $(echo "$COMPLETION_RATE < 50" | bc -l) )); then
  echo "CRITICAL: Completion rate below 50%: $COMPLETION_RATE%" | mail -s "Beta Alert: Low Completion" team@example.com
fi

if (( $(echo "$AVG_RATING < 5" | bc -l) )); then
  echo "CRITICAL: Average rating below 5: $AVG_RATING/10" | mail -s "Beta Alert: Low Satisfaction" team@example.com
fi
```

### Slack Integration

```typescript
// slack-alerts.ts
import axios from 'axios';

async function sendSlackAlert(message: string, severity: 'info' | 'warning' | 'critical') {
  const colors = {
    info: '#36a64f',
    warning: '#ff9900',
    critical: '#ff0000',
  };

  await axios.post(process.env.SLACK_WEBHOOK_URL!, {
    attachments: [{
      color: colors[severity],
      title: `Beta Metrics Alert - ${severity.toUpperCase()}`,
      text: message,
      ts: Math.floor(Date.now() / 1000),
    }],
  });
}

// Usage
if (activationRate < 40) {
  await sendSlackAlert(
    `Activation rate dropped to ${activationRate}%. Immediate action required.`,
    'critical'
  );
}
```

## Dashboard Visualization

### Recommended Tools

**Option 1: Grafana**
- Connect to PostgreSQL
- Create custom dashboards
- Set up alerts
- Share with team

**Option 2: Metabase**
- Easy setup
- SQL-based queries
- Beautiful visualizations
- Shareable dashboards

**Option 3: Custom Dashboard**
- Build with React/Next.js
- Real-time updates
- Custom visualizations
- Embedded in admin panel

### Key Dashboard Panels

1. **Overview Panel**
   - Total beta testers
   - Activation rate
   - Active users (7-day)
   - Average satisfaction

2. **Engagement Panel**
   - Projects created (daily)
   - Active users trend
   - Feature adoption rates
   - Completion rate trend

3. **Quality Panel**
   - Average ratings by feature
   - Satisfaction distribution
   - Quality trend over time
   - Would-recommend rate

4. **Support Panel**
   - Open tickets
   - Response time trend
   - Resolution time trend
   - Common issues

5. **Usage Panel**
   - Language pair distribution
   - Peak usage hours
   - Tier distribution
   - Geographic distribution

## Reporting Templates

### Daily Standup Report

```
Beta Program Daily Update - [Date]

üìä Key Metrics:
- New Activations: X
- Projects Created: X
- Avg Satisfaction: X/10
- Open Tickets: X

üéØ Today's Focus:
- [Priority 1]
- [Priority 2]

‚ö†Ô∏è Issues:
- [Issue 1]
- [Issue 2]

‚úÖ Wins:
- [Win 1]
- [Win 2]
```

### Weekly Summary Report

```
Beta Program Weekly Summary - Week [X]

üìà Growth:
- Total Beta Testers: X (+Y from last week)
- Activation Rate: X% (target: 70%)
- Active Users: X (X% of total)

üéØ Engagement:
- Projects Created: X (+Y from last week)
- Avg Projects/User: X
- Completion Rate: X%

‚≠ê Quality:
- Avg Satisfaction: X/10
- Would Recommend: X%
- Top Rated Feature: [Feature]

üé´ Support:
- Total Tickets: X
- Avg Response Time: X hours
- Top Issue: [Issue]

üí° Insights:
- [Insight 1]
- [Insight 2]

üìã Next Week:
- [Action 1]
- [Action 2]
```

### Monthly Executive Report

```
Beta Program Monthly Report - [Month Year]

üéØ Program Status: [On Track / At Risk / Behind]

üìä Key Achievements:
- [Achievement 1]
- [Achievement 2]
- [Achievement 3]

üìà Metrics Summary:
- Beta Testers: X (target: Y)
- Activation Rate: X% (target: 70%)
- Active Users: X% (target: 60%)
- Avg Satisfaction: X/10 (target: 8+)
- Completion Rate: X% (target: 80%)

üí° Key Learnings:
- [Learning 1]
- [Learning 2]
- [Learning 3]

üöÄ Product Improvements:
- [Improvement 1]
- [Improvement 2]
- [Improvement 3]

‚ö†Ô∏è Challenges:
- [Challenge 1]: [Mitigation]
- [Challenge 2]: [Mitigation]

üìã Next Month Focus:
- [Focus 1]
- [Focus 2]
- [Focus 3]

üí∞ ROI:
- Bugs Found: X
- Features Validated: X
- User Insights: X
- Estimated Value: $X
```

## Best Practices

### 1. Regular Monitoring
- Check metrics daily
- Deep dive weekly
- Comprehensive review monthly
- Share insights with team

### 2. Action-Oriented
- Don't just collect data
- Set clear thresholds
- Define actions for each threshold
- Follow through on actions

### 3. Trend Analysis
- Look at trends, not just snapshots
- Compare week-over-week
- Identify patterns
- Predict future issues

### 4. Segment Analysis
- Analyze by user type
- Compare cohorts
- Identify power users
- Understand inactive users

### 5. Feedback Loop
- Share metrics with beta testers
- Show impact of their feedback
- Celebrate improvements
- Build transparency

## Troubleshooting

### Metrics Not Updating

**Check:**
- Database connection
- Prisma client generated
- Recent data exists
- No query errors

**Fix:**
```bash
npx prisma generate
npm run beta-metrics display
```

### Slow Query Performance

**Optimize:**
- Add database indexes
- Limit date ranges
- Cache results
- Use materialized views

**Example Index:**
```sql
CREATE INDEX idx_projects_user_status ON projects(user_id, status);
CREATE INDEX idx_feedback_user_rating ON feedback(user_id, rating);
CREATE INDEX idx_support_tickets_user_status ON support_tickets(user_id, status);
```

### Inaccurate Metrics

**Verify:**
- Data quality
- Query logic
- Date ranges
- Filters applied

**Debug:**
```bash
# Check raw data
npm run beta-metrics json | jq '.activation'

# Run specific queries
psql -d your_db -c "SELECT COUNT(*) FROM users WHERE is_beta_tester = true;"
```

## Next Steps

1. **Set Up Monitoring**
   - Run initial metrics report
   - Set up daily monitoring
   - Create dashboard (optional)
   - Configure alerts

2. **Establish Baselines**
   - Record initial metrics
   - Set realistic targets
   - Define thresholds
   - Plan improvements

3. **Regular Reviews**
   - Schedule daily checks
   - Weekly team reviews
   - Monthly stakeholder updates
   - Quarterly program assessment

4. **Continuous Improvement**
   - Act on insights
   - Iterate on metrics
   - Refine thresholds
   - Optimize monitoring

---

**Last Updated:** [Date]
**Owner:** Beta Program Manager
**Contact:** beta@example.com

