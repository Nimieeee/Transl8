# GKE Cluster Configuration with GPU Node Pools
# This configuration sets up a Google Kubernetes Engine cluster with GPU-enabled nodes
# for running AI model inference workloads

apiVersion: container.cnrm.cloud.google.com/v1beta1
kind: ContainerCluster
metadata:
  name: ai-dubbing-production
  namespace: default
spec:
  location: us-central1
  initialNodeCount: 1
  
  # Enable autoscaling at cluster level
  clusterAutoscaling:
    enabled: true
    autoscalingProfile: OPTIMIZE_UTILIZATION
    resourceLimits:
      - resourceType: cpu
        minimum: 4
        maximum: 100
      - resourceType: memory
        minimum: 16
        maximum: 400
      - resourceType: nvidia-tesla-a100
        minimum: 0
        maximum: 8
      - resourceType: nvidia-tesla-v100
        minimum: 0
        maximum: 16
  
  # Workload identity for secure service account access
  workloadIdentityConfig:
    workloadPool: PROJECT_ID.svc.id.goog
  
  # Network configuration
  networkConfig:
    enableIntraNodeVisibility: true
  
  # Monitoring and logging
  loggingService: logging.googleapis.com/kubernetes
  monitoringService: monitoring.googleapis.com/kubernetes
  
  # Maintenance window
  maintenancePolicy:
    window:
      recurringWindow:
        window:
          startTime: "2024-01-01T00:00:00Z"
          endTime: "2024-01-01T04:00:00Z"
        recurrence: "FREQ=WEEKLY;BYDAY=SU"
  
  # Release channel for automatic updates
  releaseChannel:
    channel: REGULAR
  
  # Enable GPU sharing (time-slicing)
  addonsConfig:
    gcePersistentDiskCsiDriverConfig:
      enabled: true

---
# GPU Node Pool for A100 GPUs (High-performance inference)
apiVersion: container.cnrm.cloud.google.com/v1beta1
kind: ContainerNodePool
metadata:
  name: gpu-a100-pool
  namespace: default
spec:
  clusterRef:
    name: ai-dubbing-production
  location: us-central1
  
  initialNodeCount: 0
  
  autoscaling:
    enabled: true
    minNodeCount: 0
    maxNodeCount: 4
    locationPolicy: BALANCED
  
  nodeConfig:
    machineType: a2-highgpu-1g  # 1x A100 GPU, 12 vCPUs, 85GB RAM
    
    guestAccelerator:
      - type: nvidia-tesla-a100
        count: 1
        gpuSharingConfig:
          maxSharedClientsPerGpu: 4  # Enable GPU time-slicing
          gpuSharingStrategy: TIME_SHARING
    
    diskSizeGb: 200
    diskType: pd-ssd
    
    # Preemptible for cost savings (can be overridden for critical workloads)
    preemptible: false
    
    oauthScopes:
      - https://www.googleapis.com/auth/cloud-platform
    
    labels:
      workload-type: ai-inference
      gpu-type: a100
      tier: premium
    
    taints:
      - key: nvidia.com/gpu
        value: "true"
        effect: NO_SCHEDULE
    
    metadata:
      disable-legacy-endpoints: "true"
    
    shieldedInstanceConfig:
      enableSecureBoot: true
      enableIntegrityMonitoring: true

---
# GPU Node Pool for V100 GPUs (Cost-effective inference)
apiVersion: container.cnrm.cloud.google.com/v1beta1
kind: ContainerNodePool
metadata:
  name: gpu-v100-pool
  namespace: default
spec:
  clusterRef:
    name: ai-dubbing-production
  location: us-central1
  
  initialNodeCount: 1
  
  autoscaling:
    enabled: true
    minNodeCount: 1
    maxNodeCount: 8
    locationPolicy: BALANCED
  
  nodeConfig:
    machineType: n1-standard-8  # 8 vCPUs, 30GB RAM
    
    guestAccelerator:
      - type: nvidia-tesla-v100
        count: 1
        gpuSharingConfig:
          maxSharedClientsPerGpu: 2
          gpuSharingStrategy: TIME_SHARING
    
    diskSizeGb: 150
    diskType: pd-standard
    
    # Use spot instances for non-critical workloads
    spot: true
    
    oauthScopes:
      - https://www.googleapis.com/auth/cloud-platform
    
    labels:
      workload-type: ai-inference
      gpu-type: v100
      tier: standard
    
    taints:
      - key: nvidia.com/gpu
        value: "true"
        effect: NO_SCHEDULE
    
    metadata:
      disable-legacy-endpoints: "true"
    
    shieldedInstanceConfig:
      enableSecureBoot: true
      enableIntegrityMonitoring: true

---
# CPU Node Pool for non-GPU workloads
apiVersion: container.cnrm.cloud.google.com/v1beta1
kind: ContainerNodePool
metadata:
  name: cpu-pool
  namespace: default
spec:
  clusterRef:
    name: ai-dubbing-production
  location: us-central1
  
  initialNodeCount: 2
  
  autoscaling:
    enabled: true
    minNodeCount: 2
    maxNodeCount: 10
    locationPolicy: BALANCED
  
  nodeConfig:
    machineType: n2-standard-4  # 4 vCPUs, 16GB RAM
    
    diskSizeGb: 100
    diskType: pd-standard
    
    preemptible: false
    
    oauthScopes:
      - https://www.googleapis.com/auth/cloud-platform
    
    labels:
      workload-type: general
      tier: standard
    
    metadata:
      disable-legacy-endpoints: "true"
    
    shieldedInstanceConfig:
      enableSecureBoot: true
      enableIntegrityMonitoring: true
