# Cost Tracking and Optimization Configuration

---
# Cost Tracking ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-config
  namespace: monitoring
data:
  # GPU instance costs per hour (update with actual pricing)
  gpu_costs.json: |
    {
      "gke": {
        "a2-highgpu-1g": 3.67,
        "n1-standard-8-v100": 2.48
      },
      "eks": {
        "p4d.24xlarge": 32.77,
        "p3.2xlarge": 3.06,
        "p3.8xlarge": 12.24
      }
    }
  
  # Cost allocation by service
  cost_allocation.json: |
    {
      "whisper-pyannote-stt": {
        "priority": "high",
        "budget_monthly": 5000,
        "alert_threshold": 0.8
      },
      "marian-mt": {
        "priority": "medium",
        "budget_monthly": 2000,
        "alert_threshold": 0.8
      },
      "styletts-tts": {
        "priority": "high",
        "budget_monthly": 5000,
        "alert_threshold": 0.8
      },
      "xtts-tts": {
        "priority": "high",
        "budget_monthly": 5000,
        "alert_threshold": 0.8
      },
      "wav2lip-lipsync": {
        "priority": "medium",
        "budget_monthly": 3000,
        "alert_threshold": 0.8
      }
    }

---
# Cost Tracking Exporter
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cost-tracking-exporter
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cost-tracking-exporter
  template:
    metadata:
      labels:
        app: cost-tracking-exporter
    spec:
      serviceAccountName: cost-tracking
      containers:
        - name: exporter
          image: your-registry/cost-tracking-exporter:latest
          ports:
            - containerPort: 9091
              name: metrics
          env:
            - name: CLOUD_PROVIDER
              value: "gke"  # or "eks"
            - name: PROMETHEUS_URL
              value: "http://prometheus:9090"
            - name: UPDATE_INTERVAL
              value: "300"  # 5 minutes
          volumeMounts:
            - name: cost-config
              mountPath: /config
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 200m
              memory: 512Mi
      volumes:
        - name: cost-config
          configMap:
            name: cost-config

---
apiVersion: v1
kind: Service
metadata:
  name: cost-tracking-exporter
  namespace: monitoring
spec:
  ports:
    - port: 9091
      targetPort: 9091
      name: metrics
  selector:
    app: cost-tracking-exporter

---
# ServiceAccount for cost tracking
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cost-tracking
  namespace: monitoring

---
# ClusterRole for cost tracking
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cost-tracking
rules:
  - apiGroups: [""]
    resources: ["nodes", "pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["deployments", "replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["metrics.k8s.io"]
    resources: ["nodes", "pods"]
    verbs: ["get", "list"]

---
# ClusterRoleBinding for cost tracking
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cost-tracking
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cost-tracking
subjects:
  - kind: ServiceAccount
    name: cost-tracking
    namespace: monitoring

---
# Prometheus Rules for Cost Tracking
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: monitoring
data:
  cost-tracking.yml: |
    groups:
      - name: cost_tracking
        interval: 5m
        rules:
          # GPU utilization by service
          - record: gpu_utilization_by_service
            expr: |
              avg by (kubernetes_namespace, app) (
                DCGM_FI_DEV_GPU_UTIL{kubernetes_namespace="ai-models"}
              )
          
          # GPU hours by service (cumulative)
          - record: gpu_hours_by_service
            expr: |
              sum by (app) (
                count_over_time(
                  DCGM_FI_DEV_GPU_UTIL{kubernetes_namespace="ai-models"}[1h]
                ) / 60
              )
          
          # Estimated cost per service per hour
          - record: estimated_cost_per_hour_by_service
            expr: |
              sum by (app) (
                kube_pod_info{namespace="ai-models"}
                * on (node) group_left(instance_type)
                  kube_node_labels{label_node_kubernetes_io_instance_type=~".*gpu.*"}
              ) * 3.0  # Average GPU instance cost
          
          # Processing time per job
          - record: processing_time_seconds_by_stage
            expr: |
              histogram_quantile(0.95,
                sum by (queue_name, le) (
                  rate(job_processing_duration_seconds_bucket[5m])
                )
              )
          
          # Cost per processing minute
          - record: cost_per_processing_minute
            expr: |
              sum by (app) (
                rate(estimated_cost_per_hour_by_service[1h])
              ) / 60
          
          # Jobs processed per hour
          - record: jobs_processed_per_hour
            expr: |
              sum by (app, queue_name) (
                rate(job_completed_total[1h]) * 3600
              )
          
          # Cost per job
          - record: cost_per_job
            expr: |
              cost_per_processing_minute
              / on (app) group_left
              jobs_processed_per_hour
          
          # Monthly cost projection
          - record: monthly_cost_projection_by_service
            expr: |
              sum by (app) (
                estimated_cost_per_hour_by_service * 24 * 30
              )
          
          # Budget utilization percentage
          - record: budget_utilization_percentage
            expr: |
              (monthly_cost_projection_by_service / 5000) * 100

---
# Cost Alerts
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-alerts
  namespace: monitoring
data:
  cost-alerts.yml: |
    groups:
      - name: cost_alerts
        interval: 5m
        rules:
          # Budget threshold alert
          - alert: BudgetThresholdExceeded
            expr: budget_utilization_percentage > 80
            for: 10m
            labels:
              severity: warning
              category: cost
            annotations:
              summary: "Service {{ $labels.app }} exceeding budget threshold"
              description: "{{ $labels.app }} is at {{ $value }}% of monthly budget"
          
          # High GPU cost alert
          - alert: HighGPUCost
            expr: estimated_cost_per_hour_by_service > 50
            for: 30m
            labels:
              severity: warning
              category: cost
            annotations:
              summary: "High GPU cost for {{ $labels.app }}"
              description: "{{ $labels.app }} is costing ${{ $value }}/hour"
          
          # Low GPU utilization alert
          - alert: LowGPUUtilization
            expr: gpu_utilization_by_service < 30
            for: 1h
            labels:
              severity: info
              category: optimization
            annotations:
              summary: "Low GPU utilization for {{ $labels.app }}"
              description: "{{ $labels.app }} GPU utilization is {{ $value }}%"
          
          # Inefficient processing alert
          - alert: HighCostPerJob
            expr: cost_per_job > 0.50
            for: 30m
            labels:
              severity: warning
              category: optimization
            annotations:
              summary: "High cost per job for {{ $labels.app }}"
              description: "{{ $labels.app }} costs ${{ $value }} per job"
          
          # Idle GPU alert
          - alert: IdleGPU
            expr: |
              sum by (node) (DCGM_FI_DEV_GPU_UTIL) < 10
              and
              count by (node) (kube_pod_info{namespace="ai-models"}) > 0
            for: 30m
            labels:
              severity: info
              category: optimization
            annotations:
              summary: "Idle GPU on node {{ $labels.node }}"
              description: "GPU on {{ $labels.node }} is idle but pods are running"

---
# Cost Optimization CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cost-optimization
  namespace: monitoring
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: cost-tracking
          containers:
            - name: optimizer
              image: your-registry/cost-optimizer:latest
              env:
                - name: PROMETHEUS_URL
                  value: "http://prometheus:9090"
                - name: SLACK_WEBHOOK_URL
                  valueFrom:
                    secretKeyRef:
                      name: alert-secrets
                      key: slack-webhook-url
                      optional: true
              command:
                - /bin/sh
                - -c
                - |
                  echo "Running cost optimization analysis..."
                  
                  # Analyze GPU utilization
                  echo "Checking GPU utilization..."
                  
                  # Get low utilization pods
                  LOW_UTIL_PODS=$(kubectl get pods -n ai-models -o json | \
                    jq -r '.items[] | select(.metadata.labels.tier=="inference") | .metadata.name')
                  
                  for pod in $LOW_UTIL_PODS; do
                    # Check if pod has low GPU utilization
                    # Recommend scaling down or using smaller GPU
                    echo "Analyzing $pod..."
                  done
                  
                  # Generate cost report
                  echo "Generating cost report..."
                  
                  # Send report (implement webhook call)
                  echo "Cost optimization analysis complete"
          restartPolicy: OnFailure

---
# Spot Instance Manager for Cost Savings
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spot-instance-manager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spot-instance-manager
  template:
    metadata:
      labels:
        app: spot-instance-manager
    spec:
      serviceAccountName: cost-tracking
      containers:
        - name: manager
          image: your-registry/spot-instance-manager:latest
          env:
            - name: CLOUD_PROVIDER
              value: "gke"  # or "eks"
            - name: CHECK_INTERVAL
              value: "300"  # 5 minutes
            - name: SPOT_PREFERENCE_THRESHOLD
              value: "0.6"  # Use spot if >60% cost savings
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi
