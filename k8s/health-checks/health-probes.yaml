# Health Check and Readiness Probe Configurations
# This file documents the health check strategy for all model services

---
# Health Check Endpoints Implementation Guide
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-check-guide
  namespace: ai-models
data:
  README.md: |
    # Health Check Implementation Guide
    
    All model services must implement the following health check endpoints:
    
    ## Endpoints
    
    ### 1. Liveness Probe: /health
    
    Checks if the service is alive and should be restarted if failing.
    
    **Response**:
    ```json
    {
      "status": "healthy",
      "timestamp": "2024-01-01T00:00:00Z",
      "service": "whisper-pyannote-stt",
      "version": "1.0.0"
    }
    ```
    
    **Failure Conditions**:
    - Service cannot respond
    - Critical dependencies unavailable (GPU, model files)
    - Unrecoverable errors
    
    ### 2. Readiness Probe: /ready
    
    Checks if the service is ready to accept traffic.
    
    **Response**:
    ```json
    {
      "status": "ready",
      "timestamp": "2024-01-01T00:00:00Z",
      "checks": {
        "model_loaded": true,
        "gpu_available": true,
        "storage_accessible": true
      }
    }
    ```
    
    **Failure Conditions**:
    - Models not loaded
    - GPU not available
    - Storage not accessible
    - High error rate
    
    ### 3. Startup Probe: /health
    
    Checks if the service has started successfully (same as liveness).
    Used during initial startup to allow longer initialization time.
    
    ## Probe Configuration
    
    ### Liveness Probe
    - Initial Delay: 120s (allow model loading)
    - Period: 30s
    - Timeout: 10s
    - Failure Threshold: 3
    
    ### Readiness Probe
    - Initial Delay: 60s
    - Period: 10s
    - Timeout: 5s
    - Failure Threshold: 3
    
    ### Startup Probe
    - Initial Delay: 30s
    - Period: 10s
    - Timeout: 5s
    - Failure Threshold: 30 (allow up to 5 minutes for startup)
    
    ## Implementation Example (Python/FastAPI)
    
    ```python
    from fastapi import FastAPI, HTTPException
    from datetime import datetime
    import torch
    
    app = FastAPI()
    
    # Global state
    model_loaded = False
    gpu_available = False
    
    @app.get("/health")
    async def health_check():
        """Liveness probe - checks if service is alive"""
        if not model_loaded:
            raise HTTPException(status_code=503, detail="Model not loaded")
        
        return {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "service": "whisper-pyannote-stt",
            "version": "1.0.0"
        }
    
    @app.get("/ready")
    async def readiness_check():
        """Readiness probe - checks if service can accept traffic"""
        checks = {
            "model_loaded": model_loaded,
            "gpu_available": torch.cuda.is_available(),
            "storage_accessible": check_storage_access()
        }
        
        if not all(checks.values()):
            raise HTTPException(
                status_code=503,
                detail={"status": "not_ready", "checks": checks}
            )
        
        return {
            "status": "ready",
            "timestamp": datetime.utcnow().isoformat(),
            "checks": checks
        }
    
    def check_storage_access():
        """Check if storage is accessible"""
        try:
            # Test S3/GCS access
            return True
        except Exception:
            return False
    ```
    
    ## Monitoring
    
    Health check metrics are exposed via Prometheus:
    - `health_check_total{status="success|failure"}`
    - `health_check_duration_seconds`
    - `readiness_check_total{status="ready|not_ready"}`

---
# Example: Enhanced Health Check Service
apiVersion: v1
kind: Service
metadata:
  name: health-check-aggregator
  namespace: ai-models
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
      name: http
  selector:
    app: health-check-aggregator

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: health-check-aggregator
  namespace: ai-models
spec:
  replicas: 1
  selector:
    matchLabels:
      app: health-check-aggregator
  template:
    metadata:
      labels:
        app: health-check-aggregator
    spec:
      containers:
        - name: aggregator
          image: your-registry/health-check-aggregator:latest
          ports:
            - containerPort: 8080
          env:
            - name: SERVICES
              value: "whisper-pyannote-stt,marian-mt,styletts-tts,xtts-tts,wav2lip-lipsync"
            - name: CHECK_INTERVAL
              value: "30s"
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi

---
# CronJob for periodic health checks and alerting
apiVersion: batch/v1
kind: CronJob
metadata:
  name: health-check-monitor
  namespace: ai-models
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: monitor
              image: curlimages/curl:latest
              command:
                - /bin/sh
                - -c
                - |
                  SERVICES="whisper-pyannote-stt marian-mt styletts-tts xtts-tts wav2lip-lipsync"
                  
                  for service in $SERVICES; do
                    echo "Checking $service..."
                    
                    if ! curl -f -s http://${service}.ai-models.svc.cluster.local:8000/health > /dev/null; then
                      echo "ERROR: $service health check failed"
                      # Send alert (implement webhook call here)
                    else
                      echo "OK: $service is healthy"
                    fi
                    
                    if ! curl -f -s http://${service}.ai-models.svc.cluster.local:8000/ready > /dev/null; then
                      echo "WARNING: $service is not ready"
                    else
                      echo "OK: $service is ready"
                    fi
                  done
          restartPolicy: OnFailure
