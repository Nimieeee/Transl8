# Horizontal Pod Autoscaler with Queue-Based Scaling
# This configuration scales model services based on job queue depth and GPU utilization

---
# Custom Metrics API Configuration for Queue Depth
apiVersion: v1
kind: ConfigMap
metadata:
  name: queue-metrics-config
  namespace: ai-models
data:
  redis_host: "redis.default.svc.cluster.local"
  redis_port: "6379"
  queue_names: "stt-queue,mt-queue,tts-queue,lipsync-queue"
  metrics_interval: "30s"

---
# HPA for Whisper + Pyannote STT Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: whisper-pyannote-stt-hpa
  namespace: ai-models
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: whisper-pyannote-stt
  minReplicas: 2
  maxReplicas: 8
  metrics:
    # Scale based on GPU utilization
    - type: Pods
      pods:
        metric:
          name: nvidia_gpu_duty_cycle
        target:
          type: AverageValue
          averageValue: "80"
    
    # Scale based on CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    
    # Scale based on memory utilization
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    
    # Scale based on queue depth (custom metric)
    - type: External
      external:
        metric:
          name: redis_queue_depth
          selector:
            matchLabels:
              queue_name: stt-queue
        target:
          type: AverageValue
          averageValue: "5"  # Scale up if >5 jobs per pod
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 120
        - type: Pods
          value: 1
          periodSeconds: 120
      selectPolicy: Min

---
# HPA for Marian MT Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: marian-mt-hpa
  namespace: ai-models
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: marian-mt
  minReplicas: 2
  maxReplicas: 6
  metrics:
    - type: Pods
      pods:
        metric:
          name: nvidia_gpu_duty_cycle
        target:
          type: AverageValue
          averageValue: "75"
    
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    
    - type: External
      external:
        metric:
          name: redis_queue_depth
          selector:
            matchLabels:
              queue_name: mt-queue
        target:
          type: AverageValue
          averageValue: "10"
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 120
      selectPolicy: Min

---
# HPA for StyleTTS Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: styletts-tts-hpa
  namespace: ai-models
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: styletts-tts
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Pods
      pods:
        metric:
          name: nvidia_gpu_duty_cycle
        target:
          type: AverageValue
          averageValue: "80"
    
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    
    - type: External
      external:
        metric:
          name: redis_queue_depth
          selector:
            matchLabels:
              queue_name: tts-queue
              voice_type: preset
        target:
          type: AverageValue
          averageValue: "5"
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 120
      selectPolicy: Min

---
# HPA for XTTS Voice Cloning Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: xtts-tts-hpa
  namespace: ai-models
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: xtts-tts
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Pods
      pods:
        metric:
          name: nvidia_gpu_duty_cycle
        target:
          type: AverageValue
          averageValue: "80"
    
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    
    - type: External
      external:
        metric:
          name: redis_queue_depth
          selector:
            matchLabels:
              queue_name: tts-queue
              voice_type: clone
        target:
          type: AverageValue
          averageValue: "3"
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 120
      selectPolicy: Min

---
# HPA for Wav2Lip Lip-Sync Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: wav2lip-lipsync-hpa
  namespace: ai-models
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: wav2lip-lipsync
  minReplicas: 1
  maxReplicas: 6
  metrics:
    - type: Pods
      pods:
        metric:
          name: nvidia_gpu_duty_cycle
        target:
          type: AverageValue
          averageValue: "75"
    
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    
    - type: External
      external:
        metric:
          name: redis_queue_depth
          selector:
            matchLabels:
              queue_name: lipsync-queue
        target:
          type: AverageValue
          averageValue: "3"
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 90
      policies:
        - type: Percent
          value: 100
          periodSeconds: 90
        - type: Pods
          value: 1
          periodSeconds: 90
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 180
      selectPolicy: Min

---
# Queue Metrics Exporter Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: queue-metrics-exporter
  namespace: ai-models
spec:
  replicas: 1
  selector:
    matchLabels:
      app: queue-metrics-exporter
  template:
    metadata:
      labels:
        app: queue-metrics-exporter
    spec:
      containers:
        - name: exporter
          image: your-registry/queue-metrics-exporter:latest
          ports:
            - name: metrics
              containerPort: 9090
          env:
            - name: REDIS_HOST
              valueFrom:
                configMapKeyRef:
                  name: queue-metrics-config
                  key: redis_host
            - name: REDIS_PORT
              valueFrom:
                configMapKeyRef:
                  name: queue-metrics-config
                  key: redis_port
            - name: QUEUE_NAMES
              valueFrom:
                configMapKeyRef:
                  name: queue-metrics-config
                  key: queue_names
            - name: METRICS_INTERVAL
              valueFrom:
                configMapKeyRef:
                  name: queue-metrics-config
                  key: metrics_interval
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi

---
apiVersion: v1
kind: Service
metadata:
  name: queue-metrics-exporter
  namespace: ai-models
  labels:
    app: queue-metrics-exporter
spec:
  ports:
    - port: 9090
      targetPort: 9090
      name: metrics
  selector:
    app: queue-metrics-exporter
